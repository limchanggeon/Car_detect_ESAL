"""
Background worker thread for video stream processing and inference
"""

import time
import math
from PyQt5 import QtCore, QtGui
from typing import Optional, Tuple, Dict
from ..core.detector import VehicleDetector, VehicleTracker

class StreamWorker(QtCore.QThread):
    """
    ë¹„ë””ì˜¤ ì†ŒìŠ¤ë¥¼ ì½ê³  ëª¨ë¸ ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ì—¬ QImageë¥¼ ë°©ì¶œí•˜ëŠ” ì›Œì»¤ ìŠ¤ë ˆë“œ
    
    roi ì†ì„±ì´ ì„¤ì •ë˜ë©´ í•´ë‹¹ ì˜ì—­ë§Œ í¬ë¡­í•´ì„œ ì¶”ë¡ í•˜ê³ , 
    ê²°ê³¼ë¥¼ ì›ë³¸ í”„ë ˆì„ì— ì˜¤ë²„ë ˆì´í•œë‹¤.
    """

    frame_ready = QtCore.pyqtSignal(object)  # QImage
    status = QtCore.pyqtSignal(str)
    count_changed = QtCore.pyqtSignal(object)  # Dict[str, int]

    def __init__(self, source: str, detector: VehicleDetector, performance_config: dict = None):
        super().__init__()
        self.source = source
        self.detector = detector
        self._running = True
        
        # ì„±ëŠ¥ ì„¤ì • (ê¸°ë³¸ê°’ ì‚¬ìš© ë˜ëŠ” ì „ë‹¬ë°›ì€ ì„¤ì •)
        self.performance_config = performance_config or {
            "sleep_time": 0.1,
            "imgsz": 640
        }
        
        # ROI: (x, y, w, h) in ì›ë³¸ í”„ë ˆì„ í”½ì…€ ì¢Œí‘œ ë˜ëŠ” None
        self.roi = None
        
        # ì°¨ëŸ‰ ì¶”ì ê¸°
        self.tracker = VehicleTracker()
        
        # FPS ì¸¡ì •ìš© ë³€ìˆ˜ë“¤
        self.fps_counter = 0
        self.fps_start_time = time.time()
        self.current_fps = 0.0

    def stop(self):
        """ì›Œì»¤ ìŠ¤ë ˆë“œ ì¤‘ì§€"""
        self._running = False

    def run(self):
        """ë©”ì¸ ì›Œì»¤ ë£¨í”„"""
        import cv2
        
        # ì†ŒìŠ¤ ì—´ê¸° ì‹œë„
        cap = cv2.VideoCapture(self.source)
        
        if not cap.isOpened():
            self.status.emit("ì†ŒìŠ¤ ì—´ê¸° ì‹¤íŒ¨")
            return

        self.status.emit("ì‹¤í–‰ ì¤‘")
        frame_count = 0
        last_fps_update = time.time()
        
        while self._running:
            ret, frame = cap.read()
            if not ret:
                # ë¹„ë””ì˜¤ íŒŒì¼ì˜ ëì— ë„ë‹¬í–ˆì„ ë•Œ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì‹œì‘
                cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
                continue

            frame_count += 1
            
            # í”„ë ˆì„ ì²˜ë¦¬ ë° íƒì§€ ìˆ˜í–‰
            annotated_frame = self._process_frame(frame)
            
            # QImageë¡œ ë³€í™˜í•˜ì—¬ ë°©ì¶œ
            qimg = self._frame_to_qimage(annotated_frame)
            if qimg is not None:
                self.frame_ready.emit(qimg)
            
            # FPS ê³„ì‚°
            current_time = time.time()
            self.fps_counter += 1
            
            # 1ì´ˆë§ˆë‹¤ FPS ì—…ë°ì´íŠ¸
            if current_time - last_fps_update >= 1.0:
                self.current_fps = self.fps_counter / (current_time - last_fps_update)
                self.fps_counter = 0
                last_fps_update = current_time
            
            # ìƒíƒœ ì—…ë°ì´íŠ¸ (ëœ ìì£¼ ì—…ë°ì´íŠ¸í•˜ì—¬ UI ë¶€í•˜ ê°ì†Œ)
            if frame_count % 30 == 0:  # 30í”„ë ˆì„ë§ˆë‹¤ í•œ ë²ˆì”©ë§Œ ì—…ë°ì´íŠ¸
                total_count = self.tracker.count
                self.status.emit(f"ğŸ¥ FPS: {self.current_fps:.1f} | í”„ë ˆì„: {frame_count} | ì¹´ìš´íŠ¸: {total_count}")
            
            # ì ì ˆí•œ í”„ë ˆì„ë ˆì´íŠ¸ ìœ ì§€ (ë¶€ë“œëŸ¬ìš´ ì¬ìƒì„ ìœ„í•´ sleep ì‹œê°„ ë‹¨ì¶•)
            sleep_time = self.performance_config.get("sleep_time", 0.03)  # 33FPS ëª©í‘œ
            time.sleep(sleep_time)

        cap.release()
        self.status.emit("ì¤‘ì§€ë¨")

    def _process_frame(self, frame) -> any:
        """í”„ë ˆì„ ì²˜ë¦¬ ë° ì°¨ëŸ‰ íƒì§€"""
        try:
            import cv2
            
            # í”„ë ˆì„ì„ ì„±ëŠ¥ ì„¤ì •ì— ë”°ë¥¸ í•´ìƒë„ë¡œ ë¦¬ì‚¬ì´ì¦ˆ (ì†ë„ ìµœì í™”)
            target_size = self.performance_config.get("imgsz", 640)
            h, w = frame.shape[:2]
            if w != target_size or h != target_size:
                frame = cv2.resize(frame, (target_size, target_size), interpolation=cv2.INTER_LINEAR)
            
            # íƒì§€ ìˆ˜í–‰
            annotated, results = self.detector.detect(frame, self.roi)
            
            # ì¹´ìš´íŒ… ë¡œì§ (roiê°€ ìˆì„ ë•Œë§Œ)
            if self.roi and results is not None:
                detections = self._extract_detections(results, self.roi)
                updated_counts = self.tracker.update(detections)
                
                # ì¹´ìš´íŠ¸ ë³€ê²½ ì‹œê·¸ë„ ë°©ì¶œ
                if updated_counts:
                    self.count_changed.emit(dict(updated_counts))
            
            return annotated
            
        except Exception as e:
            print(f"[StreamWorker] í”„ë ˆì„ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return frame

    def _extract_detections(self, results, roi) -> list:
        """YOLO ê²°ê³¼ì—ì„œ íƒì§€ëœ ê°ì²´ì˜ ì¤‘ì‹¬ì ê³¼ í´ë˜ìŠ¤ ì¶”ì¶œ"""
        detections = []
        
        try:
            boxes = getattr(results[0], 'boxes', None)
            names = getattr(results[0], 'names', {})
            
            if boxes is not None and hasattr(boxes, 'xyxy'):
                xyxy = boxes.xyxy.tolist() if hasattr(boxes.xyxy, 'tolist') else []
                cls_list = boxes.cls.tolist() if hasattr(boxes, 'cls') and hasattr(boxes.cls, 'tolist') else []
                
                for i, bbox in enumerate(xyxy):
                    if len(bbox) < 4:
                        continue
                        
                    x1, y1, x2, y2 = bbox[:4]
                    cx = x1 + (x2 - x1) / 2.0
                    cy = y1 + (y2 - y1) / 2.0
                    
                    # ROI ì˜¤í”„ì…‹ì„ ê³ ë ¤í•˜ì—¬ ì›ë³¸ í”„ë ˆì„ ì¢Œí‘œë¡œ ë³€í™˜
                    orig_cx = roi[0] + cx
                    orig_cy = roi[1] + cy
                    
                    # í´ë˜ìŠ¤ ì´ë¦„ ì¶”ì¶œ
                    class_name = 'unknown'
                    if i < len(cls_list):
                        cls_idx = int(cls_list[i])
                        class_name = str(names.get(cls_idx, 'unknown'))
                    
                    detections.append((orig_cx, orig_cy, class_name))
                    
        except Exception as e:
            print(f"[StreamWorker] íƒì§€ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        
        return detections

    def _frame_to_qimage(self, frame) -> Optional[QtGui.QImage]:
        """OpenCV í”„ë ˆì„ì„ QImageë¡œ ë³€í™˜"""
        try:
            import cv2
            
            # BGR to RGB ë³€í™˜
            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            h, w, ch = rgb.shape
            bytes_per_line = rgb.strides[0]
            
            qimg = QtGui.QImage(
                rgb.data, w, h, bytes_per_line, QtGui.QImage.Format_RGB888
            )
            
            # QImageê°€ ìì²´ ë²„í¼ë¥¼ ì†Œìœ í•˜ë„ë¡ ë³µì‚¬
            return qimg.copy()
            
        except Exception as e:
            print(f"[StreamWorker] QImage ë³€í™˜ ì˜¤ë¥˜: {e}")
            try:
                # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ë¡œ í´ë°±
                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                h, w = gray.shape
                qimg = QtGui.QImage(gray.data, w, h, w, QtGui.QImage.Format_Grayscale8)
                return qimg.copy()
            except:
                return None

    def reset_count(self):
        """ì¹´ìš´íŠ¸ ë¦¬ì…‹"""
        self.tracker.reset()