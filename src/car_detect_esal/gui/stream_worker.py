"""
Background worker thread for video stream processing and inference
"""

import time
import math
from PyQt5 import QtCore, QtGui
from typing import Optional, Tuple, Dict
from ..core.detector import VehicleDetector, VehicleTracker
from ..database import TrafficDatabaseManager

class StreamWorker(QtCore.QThread):
    """
    ë¹„ë””ì˜¤ ì†ŒìŠ¤ë¥¼ ì½ê³  ëª¨ë¸ ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ì—¬ QImageë¥¼ ë°©ì¶œí•˜ëŠ” ì›Œì»¤ ìŠ¤ë ˆë“œ
    
    roi ì†ì„±ì´ ì„¤ì •ë˜ë©´ í•´ë‹¹ ì˜ì—­ë§Œ í¬ë¡­í•´ì„œ ì¶”ë¡ í•˜ê³ , 
    ê²°ê³¼ë¥¼ ì›ë³¸ í”„ë ˆì„ì— ì˜¤ë²„ë ˆì´í•œë‹¤.
    """

    frame_ready = QtCore.pyqtSignal(object)  # QImage
    status = QtCore.pyqtSignal(str)
    count_changed = QtCore.pyqtSignal(object)  # Dict[str, int]

    def __init__(self, source: str, detector: VehicleDetector, performance_config: dict = None, 
                 db_manager: TrafficDatabaseManager = None, camera_id: str = None):
        super().__init__()
        self.source = source
        self.detector = detector
        self._running = True
        
        # ì„±ëŠ¥ ì„¤ì • (ê¸°ë³¸ê°’ ì‚¬ìš© ë˜ëŠ” ì „ë‹¬ë°›ì€ ì„¤ì •)
        self.performance_config = performance_config or {
            "sleep_time": 0.1,
            "imgsz": 640
        }
        
        # ROI: (x, y, w, h) in ì›ë³¸ í”„ë ˆì„ í”½ì…€ ì¢Œí‘œ ë˜ëŠ” None
        self.roi = None
        
        # ì°¨ëŸ‰ ì¶”ì ê¸°
        self.tracker = VehicleTracker()
        
        # ë°ì´í„°ë² ì´ìŠ¤ ê´€ë ¨
        self.db_manager = db_manager
        self.camera_id = camera_id or f"cam_{int(time.time())}"
        self.detection_buffer = []  # íƒì§€ ê²°ê³¼ ë²„í¼
        self.last_db_save = time.time()
        
        # FPS ì¸¡ì •ìš© ë³€ìˆ˜ë“¤
        self.fps_counter = 0
        self.fps_start_time = time.time()
        self.current_fps = 0.0

    def stop(self):
        """ì›Œì»¤ ìŠ¤ë ˆë“œ ì¤‘ì§€"""
        self._running = False

    def run(self):
        """ë©”ì¸ ì›Œì»¤ ë£¨í”„"""
        import cv2
        
        # ì†ŒìŠ¤ ì—´ê¸° ì‹œë„
        cap = cv2.VideoCapture(self.source)
        
        if not cap.isOpened():
            self.status.emit("ì†ŒìŠ¤ ì—´ê¸° ì‹¤íŒ¨")
            return

        self.status.emit("ì‹¤í–‰ ì¤‘")
        frame_count = 0
        last_fps_update = time.time()
        
        while self._running:
            ret, frame = cap.read()
            if not ret:
                # ë¹„ë””ì˜¤ íŒŒì¼ì˜ ëì— ë„ë‹¬í–ˆì„ ë•Œ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì‹œì‘
                cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
                continue

            frame_count += 1
            
            # í”„ë ˆì„ ì²˜ë¦¬ ë° íƒì§€ ìˆ˜í–‰
            annotated_frame = self._process_frame(frame)
            
            # QImageë¡œ ë³€í™˜í•˜ì—¬ ë°©ì¶œ
            qimg = self._frame_to_qimage(annotated_frame)
            if qimg is not None:
                self.frame_ready.emit(qimg)
            
            # FPS ê³„ì‚°
            current_time = time.time()
            self.fps_counter += 1
            
            # 1ì´ˆë§ˆë‹¤ FPS ì—…ë°ì´íŠ¸
            if current_time - last_fps_update >= 1.0:
                self.current_fps = self.fps_counter / (current_time - last_fps_update)
                self.fps_counter = 0
                last_fps_update = current_time
            
            # ìƒíƒœ ì—…ë°ì´íŠ¸ (ëœ ìì£¼ ì—…ë°ì´íŠ¸í•˜ì—¬ UI ë¶€í•˜ ê°ì†Œ)
            if frame_count % 30 == 0:  # 30í”„ë ˆì„ë§ˆë‹¤ í•œ ë²ˆì”©ë§Œ ì—…ë°ì´íŠ¸
                total_count = self.tracker.count
                self.status.emit(f"ğŸ¥ FPS: {self.current_fps:.1f} | í”„ë ˆì„: {frame_count} | ì¹´ìš´íŠ¸: {total_count}")
            
            # ì ì ˆí•œ í”„ë ˆì„ë ˆì´íŠ¸ ìœ ì§€ (ë¶€ë“œëŸ¬ìš´ ì¬ìƒì„ ìœ„í•´ sleep ì‹œê°„ ë‹¨ì¶•)
            sleep_time = self.performance_config.get("sleep_time", 0.03)  # 33FPS ëª©í‘œ
            time.sleep(sleep_time)

        cap.release()
        self.status.emit("ì¤‘ì§€ë¨")

    def _process_frame(self, frame) -> any:
        """í”„ë ˆì„ ì²˜ë¦¬ ë° ì°¨ëŸ‰ íƒì§€"""
        try:
            import cv2
            
            # í”„ë ˆì„ì„ ì„±ëŠ¥ ì„¤ì •ì— ë”°ë¥¸ í•´ìƒë„ë¡œ ë¦¬ì‚¬ì´ì¦ˆ (ì†ë„ ìµœì í™”)
            target_size = self.performance_config.get("imgsz", 640)
            h, w = frame.shape[:2]
            original_frame_size = (w, h)  # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ìš© ì›ë³¸ í¬ê¸°
            
            if w != target_size or h != target_size:
                frame = cv2.resize(frame, (target_size, target_size), interpolation=cv2.INTER_LINEAR)
            
            # íƒì§€ ìˆ˜í–‰
            annotated, results = self.detector.detect(frame, self.roi)
            
            # íƒì§€ ê²°ê³¼ë¥¼ ì¶”ì  ì‹œìŠ¤í…œì— ì „ë‹¬í•˜ê³  ìƒˆë¡œìš´ ê°ì²´ë§Œ DBì— ì €ì¥
            if results is not None:
                detections = self._extract_detections_with_bbox(results, original_frame_size)
                
                # ì¶”ì ê¸° ì—…ë°ì´íŠ¸ - ìƒˆë¡œìš´ ê°ì²´ë§Œ ë°˜í™˜
                updated_counts, new_detections = self.tracker.update(detections)
                
                # ìƒˆë¡œìš´ ê°ì²´ë§Œ DBì— ì €ì¥
                if new_detections and self.db_manager:
                    self._save_new_detections_to_db(new_detections)
                
                # ì¹´ìš´íŠ¸ ë³€ê²½ ì‹œê·¸ë„ ë°©ì¶œ
                if updated_counts:
                    self.count_changed.emit(dict(updated_counts))
            
            return annotated
            
        except Exception as e:
            print(f"[StreamWorker] í”„ë ˆì„ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return frame

    def _extract_detections_with_bbox(self, results, original_frame_size) -> list:
        """YOLO ê²°ê³¼ì—ì„œ íƒì§€ëœ ê°ì²´ì˜ ì „ì²´ ì •ë³´ ì¶”ì¶œ (ì¶”ì  ë° DB ì €ì¥ìš©)"""
        detections = []
        
        try:
            boxes = getattr(results[0], 'boxes', None)
            names = getattr(results[0], 'names', {})
            
            if boxes is None or not hasattr(boxes, 'xyxy'):
                return detections
                
            xyxy = boxes.xyxy.tolist() if hasattr(boxes.xyxy, 'tolist') else []
            cls_list = boxes.cls.tolist() if hasattr(boxes, 'cls') and hasattr(boxes.cls, 'tolist') else []
            conf_list = boxes.conf.tolist() if hasattr(boxes, 'conf') and hasattr(boxes.conf, 'tolist') else []
            
            frame_width, frame_height = original_frame_size
            
            for i, bbox in enumerate(xyxy):
                if len(bbox) < 4:
                    continue
                    
                x1, y1, x2, y2 = bbox[:4]
                
                # ì¤‘ì‹¬ì  ê³„ì‚°
                cx = (x1 + x2) / 2.0
                cy = (y1 + y2) / 2.0
                
                # ROI ì˜¤í”„ì…‹ ì ìš© (ROIê°€ ìˆëŠ” ê²½ìš°)
                if self.roi:
                    cx += self.roi[0]
                    cy += self.roi[1]
                
                # í´ë˜ìŠ¤ ë° ì‹ ë¢°ë„
                vehicle_class = int(cls_list[i]) if i < len(cls_list) else 0
                vehicle_type = str(names.get(vehicle_class, 'unknown'))
                confidence = float(conf_list[i]) if i < len(conf_list) else 0.0
                
                # ë‚®ì€ ì‹ ë¢°ë„ëŠ” ì œì™¸
                if confidence < 0.5:
                    continue
                
                # ì •ê·œí™”ëœ ì¢Œí‘œ ê³„ì‚° (0.0 ~ 1.0)
                norm_x = cx / frame_width
                norm_y = cy / frame_height
                norm_width = (x2 - x1) / frame_width
                norm_height = (y2 - y1) / frame_height
                
                # ì°¨ëŸ‰ íƒ€ì… ë§¤í•‘
                vehicle_type_map = {
                    'car': 'car',
                    'motorcycle': 'motorbike',
                    'bus': 'bus',
                    'truck': 'truck',
                    'bicycle': 'motorbike',
                    'van': 'van'
                }
                
                standardized_type = vehicle_type_map.get(vehicle_type.lower(), 'car')
                
                # bbox ë°ì´í„°
                bbox_data = {
                    'vehicle_type': standardized_type,
                    'vehicle_class': vehicle_class,
                    'bbox_x': norm_x,
                    'bbox_y': norm_y,
                    'bbox_width': norm_width,
                    'bbox_height': norm_height
                }
                
                # (ì¤‘ì‹¬x, ì¤‘ì‹¬y, í´ë˜ìŠ¤ëª…, ì‹ ë¢°ë„, bboxë°ì´í„°) í˜•íƒœë¡œ ë°˜í™˜
                detections.append((cx, cy, standardized_type, confidence, bbox_data))
                    
        except Exception as e:
            print(f"[StreamWorker] íƒì§€ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        
        return detections

    def _save_new_detections_to_db(self, new_detections):
        """ìƒˆë¡œ ë°œê²¬ëœ ê°ì²´ë§Œ DBì— ì €ì¥ (ì¤‘ë³µ ë°©ì§€)"""
        try:
            if not self.db_manager or not new_detections:
                return
            
            # ë²„í¼ì— ì¶”ê°€
            self.detection_buffer.extend(new_detections)
            
            # ë²„í¼ê°€ ê°€ë“ ì°¨ê±°ë‚˜ ì¼ì • ì‹œê°„ì´ ì§€ë‚˜ë©´ ì €ì¥
            current_time = time.time()
            buffer_full = len(self.detection_buffer) >= 20  # 20ê°œì”© ë°°ì¹˜ ì €ì¥ (ì´ì „ 50ì—ì„œ ê°ì†Œ)
            time_to_save = (current_time - self.last_db_save) >= 10  # 10ì´ˆë§ˆë‹¤ ì €ì¥ (ì´ì „ 30ì´ˆì—ì„œ ê°ì†Œ)
            
            if (buffer_full or time_to_save) and self.detection_buffer:
                try:
                    success = self.db_manager.record_vehicle_detection(
                        self.camera_id, 
                        self.detection_buffer
                    )
                    
                    if success:
                        saved_count = len(self.detection_buffer)
                        self.detection_buffer.clear()
                        self.last_db_save = current_time
                        print(f"[DB] âœ… {saved_count}ê°œì˜ ìƒˆë¡œìš´ ì°¨ëŸ‰ ì €ì¥ ì™„ë£Œ (ì¤‘ë³µ ì œê±°ë¨)")
                    else:
                        print("[DB] âŒ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨")
                        
                except Exception as e:
                    print(f"[DB] ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
                    
        except Exception as e:
            print(f"[StreamWorker] DB ì €ì¥ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

    def _frame_to_qimage(self, frame) -> Optional[QtGui.QImage]:
        """OpenCV í”„ë ˆì„ì„ QImageë¡œ ë³€í™˜"""
        try:
            import cv2
            
            # BGR to RGB ë³€í™˜
            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            h, w, ch = rgb.shape
            bytes_per_line = rgb.strides[0]
            
            qimg = QtGui.QImage(
                rgb.data, w, h, bytes_per_line, QtGui.QImage.Format_RGB888
            )
            
            # QImageê°€ ìì²´ ë²„í¼ë¥¼ ì†Œìœ í•˜ë„ë¡ ë³µì‚¬
            return qimg.copy()
            
        except Exception as e:
            print(f"[StreamWorker] QImage ë³€í™˜ ì˜¤ë¥˜: {e}")
            try:
                # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ë¡œ í´ë°±
                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                h, w = gray.shape
                qimg = QtGui.QImage(gray.data, w, h, w, QtGui.QImage.Format_Grayscale8)
                return qimg.copy()
            except:
                return None

    def reset_count(self):
        """ì¹´ìš´íŠ¸ ë¦¬ì…‹"""
        self.tracker.reset()